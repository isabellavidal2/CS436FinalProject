{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNUhokVWSSz6/8krWxt+wzl"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDKf2N-FcOEL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "#selecting gpu instead of cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# Setting random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "Using device: cpu"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class ESM_Tracker:\n",
    "    #track Estimation Shift Magnitude during training\n",
    "    def __init__(self):\n",
    "        self.expected_stats = {}\n",
    "        self.estimated_stats = {}\n",
    "        self.esm_history = {}\n",
    "\n",
    "    def update_expected(self, layer_name, input_data):\n",
    "        \"\"\"Calculate expected statistics on current batch\"\"\"\n",
    "        if layer_name not in self.expected_stats:\n",
    "            self.expected_stats[layer_name] = {'mean': [], 'var': []}\n",
    "\n",
    "        #calculate the statistics for current batch (what we \"expect\")\n",
    "        if len(input_data.shape) == 4:  # CNN case\n",
    "            mean = input_data.mean(dim=(0, 2, 3))\n",
    "            var = input_data.var(dim=(0, 2, 3))\n",
    "        else:  # MLP case\n",
    "            mean = input_data.mean(dim=0)\n",
    "            var = input_data.var(dim=0)\n",
    "\n",
    "        self.expected_stats[layer_name]['mean'].append(mean.detach())\n",
    "        self.expected_stats[layer_name]['var'].append(var.detach())\n",
    "\n",
    "    def update_estimated(self, layer_name, bn_layer):\n",
    "        \"\"\"Get estimated statistics from BN running averages\"\"\"\n",
    "        if layer_name not in self.estimated_stats:\n",
    "            self.estimated_stats[layer_name] = {'mean': [], 'var': []}\n",
    "\n",
    "        self.estimated_stats[layer_name]['mean'].append(bn_layer.running_mean.detach().clone())\n",
    "        self.estimated_stats[layer_name]['var'].append(bn_layer.running_var.detach().clone())\n",
    "\n",
    "    def calculate_esm(self, layer_name):\n",
    "        \"\"\"Calculate Estimation Shift Magnitude\"\"\"\n",
    "        if layer_name not in self.expected_stats or layer_name not in self.estimated_stats:\n",
    "            return 0, 0\n",
    "\n",
    "        #use the most recent statistics\n",
    "        exp_mean = self.expected_stats[layer_name]['mean'][-1]\n",
    "        exp_var = self.expected_stats[layer_name]['var'][-1]\n",
    "        est_mean = self.estimated_stats[layer_name]['mean'][-1]\n",
    "        est_var = self.estimated_stats[layer_name]['var'][-1]\n",
    "\n",
    "        esm_mean = torch.norm(exp_mean - est_mean, p=2).item()\n",
    "        esm_var = torch.norm(torch.sqrt(exp_var + 1e-5) - torch.sqrt(est_var + 1e-5), p=2).item()\n",
    "\n",
    "        #store history\n",
    "        if layer_name not in self.esm_history:\n",
    "            self.esm_history[layer_name] = {'mean': [], 'var': []}\n",
    "\n",
    "        self.esm_history[layer_name]['mean'].append(esm_mean)\n",
    "        self.esm_history[layer_name]['var'].append(esm_var)\n",
    "\n",
    "        return esm_mean, esm_var\n",
    "\n",
    "class XBNBlock(nn.Module):\n",
    "    \"\"\"XBNBlock with BFN at position 2 (P2)\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, bfn_type='GN'):\n",
    "        super(XBNBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels//4, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels//4)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels//4, out_channels//4, 3, stride=stride, padding=1, bias=False)\n",
    "        #this is where we put BFN instead of BN (XBNBlock-P2, meaning Position 2 placement)\n",
    "        if bfn_type == 'GN':\n",
    "            self.norm2 = nn.GroupNorm(32, out_channels//4)  #GroupNorm as BFN\n",
    "        else:  #InstanceNorm\n",
    "            self.norm2 = nn.InstanceNorm2d(out_channels//4)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_channels//4, out_channels, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.norm2(self.conv2(out)))  #BFN here\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ],
   "metadata": {
    "id": "KIxb9PhxccDF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, use_xbn=False):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.use_xbn = use_xbn\n",
    "        self.esm_tracker = ESM_Tracker()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        if use_xbn:\n",
    "            self.block1 = XBNBlock(64, 128)\n",
    "            self.block2 = XBNBlock(128, 256)\n",
    "        else:\n",
    "            #create proper residual blocks for vanilla model\n",
    "            self.block1 = self._make_vanilla_res_block(64, 128)\n",
    "            self.block2 = self._make_vanilla_res_block(128, 256)\n",
    "\n",
    "        self.fc = nn.Linear(256 * 8 * 8, 10)\n",
    "        self.layers_to_track = ['bn1', 'block1.bn1', 'block1.bn3', 'block2.bn1', 'block2.bn3']\n",
    "\n",
    "    def _make_vanilla_res_block(self, in_channels, out_channels):\n",
    "        \"\"\"Create a residual block similar to XBNBlock but with all BNs\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels//4, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels//4, out_channels//4, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels//4),  # This will be position P2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels//4, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, track_esm=False, epoch=0):\n",
    "        #Layer 1\n",
    "        out = self.conv1(x)\n",
    "        if track_esm:\n",
    "            self.esm_tracker.update_expected('bn1', out)\n",
    "        out = self.bn1(out)\n",
    "        if track_esm:\n",
    "            self.esm_tracker.update_estimated('bn1', self.bn1)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "\n",
    "        #Blocks\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "\n",
    "        out = F.adaptive_avg_pool2d(out, (8, 8))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=50, model_name='vanilla'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    esm_history = {name: [] for name in model.layers_to_track}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data, track_esm=(batch_idx % 10 == 0))  # Track ESM periodically\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        # Calculate ESM at end of epoch\n",
    "        if epoch % 5 == 0:  # Track ESM every 5 epochs\n",
    "            with torch.no_grad():\n",
    "        # Simple ESM tracking for just the first BN layer\n",
    "                if hasattr(model, 'bn1'):\n",
    "             # Use a dummy forward pass to get expected stats\n",
    "                    dummy_data = next(iter(train_loader))[0][:1]  # Single sample\n",
    "                    _ = model(dummy_data, track_esm=True)\n",
    "                    esm_m, esm_v = model.esm_tracker.calculate_esm('bn1')\n",
    "                    esm_history['bn1'].append(esm_v)\n",
    "\n",
    "        # Test accuracy\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        test_accuracies.append(accuracy)\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Loss = {train_losses[-1]:.4f}, Test Acc = {accuracy:.2f}%')\n",
    "\n",
    "    return train_losses, test_accuracies, esm_history"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_robustness(model, test_loader, noise_levels=[0.0, 0.1, 0.2, 0.3, 0.4]):\n",
    "    \"\"\"Test model robustness to noise in BN statistics\"\"\"\n",
    "    original_stats = {}\n",
    "\n",
    "    # Save original BN statistics\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            original_stats[name] = {\n",
    "                'running_mean': module.running_mean.clone(),\n",
    "                'running_var': module.running_var.clone()\n",
    "            }\n",
    "\n",
    "    accuracies = []\n",
    "    confidences = []  # We'll track predictive entropy\n",
    "\n",
    "    for noise_magnitude in noise_levels:\n",
    "        # Add noise to BN statistics\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                module.running_mean = original_stats[name]['running_mean'] * (\n",
    "                    1 + torch.randn_like(original_stats[name]['running_mean']) * noise_magnitude\n",
    "                )\n",
    "                module.running_var = original_stats[name]['running_var'] * (\n",
    "                    1 + torch.randn_like(original_stats[name]['running_var']) * noise_magnitude\n",
    "                )\n",
    "\n",
    "        # Test accuracy with noisy stats\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_entropies = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "\n",
    "                # Calculate predictive entropy\n",
    "                entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-8), dim=1)\n",
    "                all_entropies.extend(entropy.cpu().numpy())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        avg_entropy = np.mean(all_entropies)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        confidences.append(avg_entropy)\n",
    "\n",
    "        print(f'Noise {noise_magnitude:.1f}: Accuracy = {accuracy:.2f}%, Avg Entropy = {avg_entropy:.4f}')\n",
    "\n",
    "    # Restore original statistics\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.running_mean = original_stats[name]['running_mean']\n",
    "            module.running_var = original_stats[name]['running_var']\n",
    "\n",
    "    return accuracies, confidences, noise_levels"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    # Data loading\n",
    "    print(\"Loading CIFAR-10 dataset...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Train vanilla model (with fewer epochs for testing)\n",
    "    print(\"\\n=== Training Vanilla CNN ===\")\n",
    "    vanilla_model = SimpleCNN(use_xbn=False)\n",
    "    vanilla_model = vanilla_model.to(device)\n",
    "    vanilla_loss, vanilla_acc, vanilla_esm = train_model(vanilla_model, train_loader, test_loader, epochs=10, model_name='vanilla')\n",
    "\n",
    "    # Train XBNBlock model\n",
    "    print(\"\\n=== Training XBNBlock CNN ===\")\n",
    "    xbn_model = SimpleCNN(use_xbn=True)\n",
    "    xbn_model = xbn_model.to(device)\n",
    "    xbn_loss, xbn_acc, xbn_esm = train_model(xbn_model, train_loader, test_loader, epochs=10, model_name='xbn')\n",
    "\n",
    "    # Test robustness\n",
    "    print(\"\\n=== Testing Robustness ===\")\n",
    "    vanilla_accuracies, vanilla_entropies, noise_levels = test_robustness(vanilla_model, test_loader)\n",
    "    xbn_accuracies, xbn_entropies, _ = test_robustness(xbn_model, test_loader)\n",
    "\n",
    "    return {\n",
    "        'vanilla': {'acc': vanilla_acc, 'loss': vanilla_loss, 'esm': vanilla_esm,\n",
    "                   'robust_acc': vanilla_accuracies, 'robust_entropy': vanilla_entropies},\n",
    "        'xbn': {'acc': xbn_acc, 'loss': xbn_loss, 'esm': xbn_esm,\n",
    "               'robust_acc': xbn_accuracies, 'robust_entropy': xbn_entropies},\n",
    "        'noise_levels': noise_levels\n",
    "    }\n",
    "\n",
    "# Run the experiment\n",
    "results = main()"
   ]
  }
 ]
}
